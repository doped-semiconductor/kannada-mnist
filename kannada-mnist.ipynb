{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/Kannada-MNIST/test.csv\n/kaggle/input/Kannada-MNIST/Dig-MNIST.csv\n/kaggle/input/Kannada-MNIST/sample_submission.csv\n/kaggle/input/Kannada-MNIST/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"traind = pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\ntestd = pd.read_csv('/kaggle/input/Kannada-MNIST/Dig-MNIST.csv')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = traind.iloc[:,:1]\nlabel_one_hot = pd.get_dummies(traind['label'])\n#print((label_one_hot))\n#print(label_one_hot.shape)\n#torch vectors:\nlabels = torch.tensor(labels.values)\nlabel1 = torch.tensor(label_one_hot.values)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labely = testd.iloc[:,:1]\n#torch vectors:\nlabely = torch.tensor(labely.values)\nlabely = labely.reshape(10240)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainx = torch.from_numpy(traind.iloc[:,1:].values).float()\ntrainx = trainx.reshape(60000,784,1)\nprint('trainx: ',trainx.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"trainx:  torch.Size([60000, 784, 1])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = labels.reshape(60000)\nprint(labels.shape)","execution_count":8,"outputs":[{"output_type":"stream","text":"torch.Size([60000])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.shape","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"torch.Size([60000, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class net(nn.Module):\n    def __init__(self):\n        super(net, self).__init__()\n               \n        self.conv1 = nn.Conv1d(784,512,1)\n        self.conv2 = nn.Conv1d(512,128,1)\n        self.conv3 = nn.Conv1d(128,64,1)\n        self.conv4 = nn.Conv1d(64,16,1)\n        self.conv5 = nn.Conv1d(16,10,1)\n        self.conv6 = nn.Conv1d(1,1,1)\n        \n        self.max1 = nn.MaxPool1d(1,3)\n        self.max2 = nn.MaxPool1d(1,7)\n        self.avg1 = nn.AvgPool1d(1,3)\n        self.avg2 = nn.AvgPool1d(1,7)\n        \n        self.batch1 = nn.BatchNorm1d(512)\n        self.batch2 = nn.BatchNorm1d(128)\n        self.batch3 = nn.BatchNorm1d(64)\n        self.batch4 = nn.BatchNorm1d(16)\n        \n        self.soft = nn.LogSoftmax(dim=1)\n    def forward(self, tx):\n        x = self.conv1(tx)\n        #print(x.shape)\n        x = self.max1(x)\n        #print(x.shape)\n        x = self.batch1(x)\n        #print(x.shape)\n        x = F.relu(x)\n        #print(x.shape)\n        \n        x = self.conv2(x)\n        #print(x.shape)\n        x = self.max2(x)\n        #print(x.shape)\n        x = self.batch2(x)\n        #print(x.shape)\n        x = F.relu(x)\n        #print(x.shape)\n\n        x = self.conv3(x)\n        #print(x.shape)\n        x = self.avg1(x)\n        #print(x.shape)\n        x = self.batch3(x)\n        #print(x.shape)\n        x = F.relu(x)\n        #print(x.shape)\n\n        x = self.conv4(x)\n        #print(x.shape)\n        x = self.avg2(x)\n        #print(x.shape)\n        x = self.batch4(x)\n        #print(x.shape)\n        x = F.relu(x)\n        #print(x.shape)\n\n        x = self.conv5(x)\n        #print(x.shape)\n        x = self.soft(x)\n        #print(x.shape)\n\n        out = x.reshape(tx.shape[0], 10)\n        return out\n        ","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = net()\ncrit = nn.NLLLoss()\nopt = optim.Adam(model.parameters(),lr = 0.09)#0.22\n\nfor i in range(1):\n    out = model(trainx)\n    loss = crit(out,labels)\n    print(i,loss.item())\n    loss.backward()\n    opt.step()\n    opt.zero_grad()","execution_count":10,"outputs":[{"output_type":"stream","text":"0 2.3924431800842285\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = net().to(device)\ncrit = nn.NLLLoss()\nopt = optim.Adam(model.parameters(),lr = 0.09)#0.22\n\ntrain_losses = []\nvalid_losses = []\n\nfor i in range(66):\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    model.train()\n    trainx = trainx.to(device)\n    labels = labels.to(device)\n    \n    opt.zero_grad()\n    out = model(trainx)\n    loss = crit(out,labels)\n    print(i,loss.item(),end = \" \")\n    loss.backward()\n    opt.step()\n    \n    model.eval()\n    testx = testx.to(device)\n    labely = labely.to(device)\n    \n    out2 = model(testx)\n    loss2 = crit(out2,labely)\n    print(loss2.item())\n    ","execution_count":19,"outputs":[{"output_type":"stream","text":"0 2.4182982444763184 103.78511047363281\n1 2.3054914474487305 80.17951965332031\n2 1.9027971029281616 66.66064453125\n3 1.6440725326538086 42.54212188720703\n4 1.413390874862671 27.824926376342773\n5 1.2177916765213013 20.31528663635254\n6 1.049516201019287 15.548977851867676\n7 0.8783397674560547 12.795978546142578\n8 0.7283703684806824 10.960580825805664\n9 0.5956215858459473 9.439802169799805\n10 0.4599405527114868 8.828069686889648\n11 0.37095746397972107 9.071538925170898\n12 0.3036683201789856 9.267646789550781\n13 0.26319125294685364 8.145739555358887\n14 0.23919591307640076 7.474677085876465\n15 0.21743015944957733 7.1408233642578125\n16 0.19571098685264587 6.882411003112793\n17 0.1812419295310974 6.664430141448975\n18 0.1743488907814026 6.494129180908203\n19 0.1686413288116455 6.376202583312988\n20 0.15974846482276917 6.274486541748047\n21 0.1488141417503357 6.139729976654053\n22 0.13980256021022797 5.942530155181885\n23 0.13344019651412964 5.69042444229126\n24 0.12847886979579926 5.417509078979492\n25 0.12294772267341614 5.163538455963135\n26 0.1167677566409111 4.955265045166016\n27 0.11097124963998795 4.781896114349365\n28 0.10639414191246033 4.613081932067871\n29 0.1023482084274292 4.438621520996094\n30 0.09848423302173615 4.263304710388184\n31 0.09448947012424469 4.0904059410095215\n32 0.09064732491970062 3.9267418384552\n33 0.08732549101114273 3.7826743125915527\n34 0.08479245007038116 3.6611926555633545\n35 0.08250605314970016 3.5561728477478027\n36 0.07992450147867203 3.4595437049865723\n37 0.07719916105270386 3.3691799640655518\n38 0.07472001761198044 3.285740375518799\n39 0.0724932849407196 3.2084126472473145\n40 0.07036109268665314 3.1336264610290527\n41 0.06826099753379822 3.061704635620117\n42 0.06613824516534805 2.9969255924224854\n43 0.06404555588960648 2.9407050609588623\n44 0.06205589324235916 2.8895606994628906\n45 0.060166291892528534 2.8425354957580566\n46 0.058322932571172714 2.801116704940796\n47 0.056607745587825775 2.765909433364868\n48 0.054994113743305206 2.73533296585083\n49 0.05341238155961037 2.708402156829834\n50 0.05185636132955551 2.686964273452759\n51 0.05040423199534416 2.6709280014038086\n52 0.04898999258875847 2.6586549282073975\n53 0.04756368324160576 2.649467945098877\n54 0.04615236446261406 2.6441874504089355\n55 0.04477250203490257 2.6411938667297363\n56 0.043443694710731506 2.6382791996002197\n57 0.04220001772046089 2.634622812271118\n58 0.041014693677425385 2.6302313804626465\n59 0.039809927344322205 2.625281572341919\n60 0.03859512507915497 2.6213812828063965\n61 0.037429019808769226 2.62056303024292\n62 0.03631560876965523 2.6224594116210938\n63 0.03520815074443817 2.6245923042297363\n64 0.034109000116586685 2.6254401206970215\n65 0.03304287791252136 2.624748706817627\n66 0.032020680606365204 2.624263048171997\n67 0.031024454161524773 2.625913381576538\n68 0.03002895787358284 2.6291961669921875\n69 0.02905207872390747 2.631676197052002\n70 0.02810780517756939 2.6320815086364746\n71 0.027182137593626976 2.632622241973877\n72 0.026286020874977112 2.635662078857422\n73 0.025412745773792267 2.640972375869751\n74 0.024551423266530037 2.646157741546631\n75 0.023709652945399284 2.650160312652588\n76 0.022883864119648933 2.6535000801086426\n77 0.022082006558775902 2.6576623916625977\n78 0.021300923079252243 2.6626644134521484\n79 0.020539870485663414 2.669489860534668\n80 0.019806019961833954 2.6796510219573975\n81 0.019094128161668777 2.6936802864074707\n82 0.018396228551864624 2.7102274894714355\n83 0.017723366618156433 2.725337266921997\n84 0.017080901190638542 2.7378976345062256\n85 0.016454841941595078 2.7480318546295166\n86 0.01584545709192753 2.758568286895752\n87 0.01526408176869154 2.770174980163574\n88 0.014719623140990734 2.7842466831207275\n89 0.014221416786313057 2.8017847537994385\n90 0.013799089007079601 2.823082447052002\n91 0.013602050952613354 2.843447685241699\n92 0.01384174358099699 2.8893115520477295\n93 0.01829976961016655 3.031548500061035\n94 0.10460279136896133 4.015970230102539\n95 0.7997468709945679 3.444336414337158\n96 0.22560159862041473 3.05322265625\n97 0.16363908350467682 3.1755356788635254\n98 0.3429536521434784 3.268634796142578\n99 0.11436571180820465 3.285745620727539\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(crit(model(testx),labely))","execution_count":13,"outputs":[{"output_type":"stream","text":"tensor(2.2766, grad_fn=<NllLossBackward>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"testx = torch.from_numpy(testd.iloc[:,1:].values).float()\nprint(testx.shape)\ntestx = testx.reshape(10240,784,1)\nprint('testx: ',testx.shape)\n","execution_count":11,"outputs":[{"output_type":"stream","text":"torch.Size([10240, 784])\ntestx:  torch.Size([10240, 784, 1])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(testx.shape)\nprint(labely.shape)\nprint(labely)","execution_count":12,"outputs":[{"output_type":"stream","text":"torch.Size([10240, 784, 1])\ntorch.Size([10240])\ntensor([0, 1, 2,  ..., 7, 8, 9])\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}